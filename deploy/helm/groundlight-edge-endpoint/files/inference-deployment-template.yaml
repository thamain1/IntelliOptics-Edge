apiVersion: v1
kind: Service
metadata:
  name: placeholder-inference-service-name
  namespace: {{ .Values.namespace }}
spec:
  selector:
    app: inference-server
    instance: placeholder-inference-instance-name
  ports:
    - protocol: TCP
      port: 8000
      name: http
      targetPort: 8000
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: placeholder-inference-deployment-name
  namespace: {{ .Values.namespace }}
  labels:
    name: placeholder-inference-deployment-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: inference-server
      instance: placeholder-inference-instance-name
  template:
    metadata:
      labels:
        app: inference-server
        instance: placeholder-inference-instance-name
    spec:
{{ if eq .Values.inferenceFlavor "gpu" }}
      runtimeClassName: nvidia  # Required for GPU use in k3s
{{- end }}
      imagePullSecrets:
      - name: registry-credentials
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 0  # Aim for no downtime during rollout

      initContainers:
      # NOTE: the sync-pinamod container is duplicated in the warmup_inference_model.yaml Job
      # TODO: refactor to share code between the Job and the initContainer in the Deployment
      - name: sync-pinamod

        command: ['sh', '-c', 'if [ -n "$MODEL_SYNC_COMMAND" ]; then eval "$MODEL_SYNC_COMMAND"; else echo "Skipping model sync; MODEL_SYNC_COMMAND not set."; fi']
        env:
        - name: MODEL_SYNC_COMMAND
          valueFrom:
            configMapKeyRef:
              name: inference-deployment-template
              key: model_sync_command
              optional: true
        - name: PINAMOD_DIR
          value: /opt/models/pinamod
        envFrom:
        - secretRef:
            name: artifact-storage-credentials
            optional: true
        volumeMounts:
        - name: pina-models
          mountPath: /opt/models

      containers:
      - name: inference-server
image: acrintellioptics.azurecr.io/intellioptics/edge-endpoint:latest
        imagePullPolicy: "{{ include "IntelliOptics-edge-endpoint.inferencePullPolicy" . }}"
        env:
        - name: MODEL_REPOSITORY
          value: &modelRepository /opt/intellioptics/edge/serving/model-repo
        - name: MODEL_NAME
          value: placeholder-model-name
        - name: PINAMOD_DIR
          value: /opt/models/pinamod
        - name: LOG_LEVEL
          value: {{ .Values.logLevel | quote }}
        - name: LOAD_ALL_PIPELINES  # Load only the pipelines that are needed for edge inference.
          value: "false"
        command:
          [
            "poetry", "run", "python3", "-m", "uvicorn", "serving.edge_inference_server.fastapi_server:app",
            "--host", "0.0.0.0",
            "--port", "8000",
            "--workers", "1"
          ]
        volumeMounts:
        - name: edge-endpoint-persistent-volume
          mountPath: *modelRepository
        - name: pina-models
          mountPath: /opt/models
          readOnly: true
        ports:
        - containerPort: 8000
          name: http-fastapi
        startupProbe:
          httpGet:
            path: /health/live  # Checks if the server is up
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 60  # Wait for up to 10 min
        readinessProbe:
          httpGet:
            path: /health/ready  # Checks if the server is ready to serve requests
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 10
          failureThreshold: 3  # after 30 seconds of failure, stop sending traffic to the pod
        livenessProbe:
          httpGet:
            # We use "ready" here because the current liveness probe is too simple and
            # doesn't check if the model is loaded.  If the models fail because of GPU memory
            # issues, we need this to fail and restart the pod.
            path: /health/ready
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 10
          failureThreshold: 6  # after 60 seconds of failure, restart the pod

      volumes:
      - name: edge-endpoint-persistent-volume
        persistentVolumeClaim:
          claimName: edge-endpoint-pvc
      - name: pina-models
        hostPath:
          path: /opt/intellioptics/edge/pinamod-public
          type: Directory
      - name: artifact-credentials
        secret:
          secretName: artifact-storage-credentials




