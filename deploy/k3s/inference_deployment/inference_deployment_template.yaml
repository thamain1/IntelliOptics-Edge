apiVersion: v1
kind: Service
metadata:
  name: placeholder-inference-service-name
spec:
  selector:
    app: inference-server
    instance: placeholder-inference-instance-name
  ports:
    - protocol: TCP
      port: 8000
      name: http
      targetPort: 8000
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: placeholder-inference-deployment-name
  labels:
    name: placeholder-inference-deployment-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: inference-server
      instance: placeholder-inference-instance-name
  template:
    metadata:
      labels:
        app: inference-server
        instance: placeholder-inference-instance-name
    spec:
      runtimeClassName: nvidia  # Required for GPU use in k3s
      imagePullSecrets:
      - name: registry-credentials
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 0  # Aim for no downtime during rollout

      initContainers:
      # NOTE: the sync-pinamod container is duplicated in the warmup_inference_model.yaml Job
      # TODO: refactor to share code between the Job and the initContainer in the Deployment
      - name: sync-pinamod


        command:
          [
            "sh",
            "-c",
            "set -euo pipefail\nrm -rf ${PINAMOD_DIR:?}/*\ncp -a ${PINAMOD_SOURCE_DIR}/. ${PINAMOD_DIR}/"
          ]
        env:
        - name: PINAMOD_DIR
          value: /opt/models/pinamod
        - name: PINAMOD_SOURCE_DIR
          value: /opt/pinamod-source

        image: mcr.microsoft.com/azure-cli:2.57.0
        command: ['sh', '-c', 'set -euo pipefail; mkdir -p "$PINAMOD_DIR" && az storage blob download-batch --connection-string "$AZURE_STORAGE_CONNECTION_STRING" --source "$PINAMOD_CONTAINER" --destination "$PINAMOD_DIR" --no-progress --overwrite true']
        env:
        - name: PINAMOD_DIR
          value: /opt/models/pinamod
        - name: PINAMOD_CONTAINER
          value: pinamod
        - name: AZURE_STORAGE_CONNECTION_STRING
          valueFrom:
            secretKeyRef:
              name: azure-service-principal
              key: AZURE_STORAGE_CONNECTION_STRING

        volumeMounts:
        - name: pina-models
          mountPath: /opt/models
        - name: pina-models-source
          mountPath: /opt/pinamod-source

      containers:
      - name: inference-server
        imagePullPolicy: Always
        env:
        - name: MODEL_REPOSITORY
          value: &modelRepository /opt/intellioptics/edge/serving/model-repo
        - name: MODEL_NAME
          value: placeholder-model-name
        - name: PINAMOD_DIR
          value: /opt/models/pinamod
        - name: LOAD_ALL_PIPELINES  # Load only the pipelines that are needed for edge inference.
          value: "false"
        command:
          [
            "poetry", "run", "python3", "-m", "uvicorn", "serving.edge_inference_server.fastapi_server:app",
            "--host", "0.0.0.0",
            "--port", "8000",
            "--workers", "1"
          ]
        volumeMounts:
        - name: edge-endpoint-persistent-volume
          mountPath: *modelRepository
        - name: pina-models
          mountPath: /opt/models
          readOnly: true
        ports:
        - containerPort: 8000
          name: http-fastapi
        startupProbe:
          httpGet:
            path: /health/live  # Checks if the server is up
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 60  # Wait for up to 10 min
        readinessProbe:
          httpGet:
            path: /health/ready  # Checks if the server is ready to serve requests
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 10
          failureThreshold: 3  # after 30 seconds of failure, stop sending traffic to the pod
        livenessProbe:
          httpGet:
            # We use "ready" here because the current liveness probe is too simple and
            # doesn't check if the model is loaded.  If the models fail because of GPU memory
            # issues, we need this to fail and restart the pod.
            path: /health/ready
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 10
          failureThreshold: 6  # after 60 seconds of failure, restart the pod

      volumes:
      - name: edge-endpoint-persistent-volume
        persistentVolumeClaim:
          claimName: edge-endpoint-pvc
      - name: pina-models
        emptyDir: {}
      - name: pina-models-source
        persistentVolumeClaim:
          claimName: pinamod-artifacts-pvc


